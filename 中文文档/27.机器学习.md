# 机器学习
1. 监督学习 vs 无监督学习

问题：区分监督学习和无监督学习，并各举一个应用场景。

答案：

* 监督学习：使用带标签数据训练模型，目标是预测输出（如分类、回归）。
    * 应用：图像分类（如 ResNet 识别猫和狗）。
* 无监督学习：从无标签数据中发现模式（如聚类、降维）。
    * 应用：客户分群（K-means 划分用户行为）。
示例代码（监督学习）：

python
from sklearn.linear_model import LogisticRegression  
X = [[2.5, 1.2], [3.1, 1.8]]  # 特征  
y = [0, 1]  # 标签  
model = LogisticRegression().fit(X, y)  
print(model.predict([[3.0, 1.5]]))  # 输出 [1]

2. 偏差 - 方差权衡

问题：解释偏差 - 方差权衡，并说明如何解决高偏差和高方差问题。

答案：
* 偏差：模型欠拟合，无法捕捉数据真实规律（如线性模型处理非线性关系）。
* 方差：模型过拟合，对噪声敏感（如高阶多项式拟合少量数据）。

解决方案：
* 高偏差：增加模型复杂度（如使用神经网络代替线性回归）。
* 高方差：正则化（L2 正则化）、增加数据量、集成学习（随机森林）。

python

from sklearn.ensemble import RandomForestRegressor  

model = RandomForestRegressor(n_estimators=100, max_depth=3)  # 降低方差

3. 过拟合与欠拟合

问题：如何判断模型过拟合？列举三种防止过拟合的方法。

答案：
* 过拟合表现：训练集准确率高，验证集准确率低。
* 防止方法：
    * 正则化（如 L1/L2 正则化）。
    * 早停（Early Stopping）。
    * 数据增强（如图像旋转、平移）。

4. 交叉验证

问题：解释 k-fold 交叉验证的原理和优点。

答案：
* 原理：将数据集分为 k 个子集，每次用 k-1 个子集训练，1 个子集验证，重复 k 次取平均。

优点：
* 更准确评估模型泛化能力。
* 有效利用数据（每个样本被训练 k-1 次）。

5. 激活函数
问题：对比 Sigmoid 和 ReLU 激活函数的优缺点。
答案：

| 激活函数 | 优点                           | 缺点                                   |
|----------|--------------------------------|----------------------------------------|
| Sigmoid  | 输出范围 0-1，适合分类         | 梯度消失、输出非零中心化               |
| ReLU     | 计算快、缓解梯度消失           | 死亡 ReLU 问题（某些神经元永不激活）   |
改进方法：
Leaky ReLU：允许负输入时有小梯度（如f(x) = max(0.1x, x)）

6. 梯度下降优化器

问题：解释 Adam 优化器的原理，并说明其超参数 β₁、β₂的作用。

答案：
原理：结合动量（Momentum）和 RMSprop，自适应调整学习率。
超参数：
* β₁（默认 0.9）：一阶矩估计的指数衰减率。
* β₂（默认 0.999）：二阶矩估计的指数衰减率


7. 特征工程

问题：列举三种常见的特征工程方法。

答案：
* 标准化 / 归一化：使特征均值为 0、方差为 1（如StandardScaler）。
* 独热编码：将类别变量转换为二进制向量（如OneHotEncoder）。
* 特征交互：创建新特征（如age²、income × education）。

8. 集成学习

问题：解释 Bagging 和 Boosting 的区别。

答案：

| 方法     | 核心思想                                                                 | 典型算法       | 并行性       |
|----------|--------------------------------------------------------------------------|----------------|--------------|
| Bagging  | 自助采样训练基模型，取平均（降低方差）                                   | 随机森林       | 支持并行     |
| Boosting | 迭代训练弱模型，调整样本权重（降低偏差）                                 | XGBoost、LightGBM | 顺序执行     |

9. 评估指标

问题：对于不平衡数据，为什么准确率不是最佳指标？应使用什么替代指标？

答案：
* 原因：准确率在正负样本比例悬殊时无法反映模型真实性能（如 99% 负样本时，预测全负准确率 99% 但无意义）。
* 替代指标：
    * 精确率 - 召回率曲线（Precision-Recall Curve）。
    * F1 分数（F1-Score）。
    * AUC-ROC 曲线（对类别不平衡不敏感）。

10. 深度学习框架
问题：对比 PyTorch 和 TensorFlow 的优缺点。
答案：

| 框架 | 优点                                                                 | 缺点               |
|-----------|-----------------------------------------------------------------------------|-----------------------------|
| PyTorch   | 动态计算图（调试方便）、灵活性高           | 部署支持较弱 |
| TensorFlow| 静态计算图（优化性能）、生态完善 | 灵活性较低         |

应用场景：
* PyTorch：研究（快速迭代）。
* TensorFlow：工业部署（模型导出、移动端支持）。

1. 迁移学习：为什么预训练模型在小数据集上表现更好？
2. 对抗生成网络（GAN）：解释生成器和判别器的博弈过程。
3. Transformer 模型：注意力机制（Attention）与循环神经网络（RNN）的区别。

答案：
1. 预训练模型已学习通用特征，小数据集只需微调最后几层即可适应新任务。
2. 生成器：生成假样本欺骗判别器；判别器：区分真样本和假样本，二者交替优化。
3. Transformer：通过自注意力捕捉长距离依赖，无循环结构；RNN：按时间步处理序列，易梯度消失

### 决策树（Decision Tree）

问题：
* 决策树如何选择最优划分特征？
* 信息增益（Information Gain）和基尼系数（Gini Index）的区别？
* 如何防止决策树过拟合？

答案：
* 通过计算特征的信息增益、信息增益比或基尼系数选择最优划分。
* 信息增益基于熵的减少，基尼系数衡量样本的不纯度，后者计算更高效。
* 剪枝（预剪枝 / 后剪枝）、限制树深度、控制叶子节点样本数。原因，贪心算法追求局部最优，导致树深度过深

核心思想：通过特征划分将样本递归分割为纯度更高的子集，最终形成树形结构。
特征选择标准：

分类树：信息增益（ID3）、信息增益比（C4.5）、基尼不纯度（CART）。

回归树：均方差（CART）

### 随机森林（Random Forest）
* 基于 Bagging（自助采样）的集成学习方法，通过构建多棵决策树投票决定最终结果。
    
关键技术：
* 自助采样（Bootstrap）：从原始数据中随机抽取样本训练每棵树。
* 随机特征子集：每棵树随机选择部分特征进行分裂，降低方差。

缺点：
* 对噪声敏感（尤其当树深度过大时）。
* 预测速度较慢（需遍历所有树）。

问题：
* 随机森林如何降低方差？
* 自助采样（Bootstrap）和特征子集的作用？
* 与梯度提升树（如 XGBoost）的区别？

答案：
* 通过集成多个决策树并取平均预测结果。通过 Bootstrap 采样和特征随机选择，训练多棵决策树并取平均预测结果。
* 通过 Bootstrap 采样和特征随机选择，训练多棵决策树并取平均预测结果。

问题：随机森林如何降低方差？

答案：通过自助采样（样本随机）和随机特征子集（特征随机）减少树之间的相关性，从而降低整体方差。

问题：如何选择n_estimators和max_depth？

答案：
* n_estimators越大，方差越小，但计算成本增加，需通过交叉验证选择平衡点。
* max_depth过大会过拟合，需结合偏差 - 方差权衡，通常设置较小值（如 5-10）。

问题：随机森林和 GBDT 的区别？

答案：
* 随机森林：Bagging 框架，并行训练，降低方差。
* GBDT：Boosting 框架，串行训练，通过梯度提升降低偏差。

随机森林中的特征重要性如何计算？
* （答案：通过计算每个特征在所有树中对节点纯度的贡献总和。）
* Bootstrap 减少过拟合，特征子集增加模型多样性。
* 随机森林并行训练基模型，梯度提升树顺序优化误差

### XGBoost（极端梯度提升树）

问题：XGBoost 相比传统 GBDT 有哪些改进？其核心参数的作用是什么？

答案：
* 改进点：
    * 二阶泰勒展开优化损失函数，收敛更快。  
    * 正则化（L1/L2）防止过拟合。
    * 并行计算特征分割点，加速训练。
* 核心参数：
    * n_estimators：树的数量，越大方差越小，但计算成本增加。
    * max_depth：树的深度，过大会过拟合，需平衡偏差和方差。
    * learning_rate：步长，降低模型复杂度。
    * subsample：样本采样比例，减少方差

* 特征重要性：使用model.feature_importances_分析特征贡献。
* 调参工具：GridSearchCV 或 XGBoost 内置的cv函数优化超参数。
* 可视化：用graphviz绘制决策树结构，辅助理解模型逻辑。

| 模型       | 优点                                  | 缺点                     | 适用场景               |
|------------|---------------------------------------|--------------------------|------------------------|
| 决策树     | 可解释性强，无需特征缩放              | 易过拟合，泛化能力差     | 快速原型开发           |
| 随机森林   | 抗过拟合能力强，支持并行训练          | 计算资源消耗大           | 大规模数据分类/回归    |
| XGBoost    | 高精度，高效处理缺失值和稀疏数据      | 调参复杂                 | 竞赛和工业级任务       |


### K-means

问题：
* K-means 的核心步骤是什么？
* 如何选择初始聚类中心？
* 如何确定最佳聚类数 K？

答案：
* 初始化中心→分配样本→更新中心→重复直到收敛。
* 随机选择或使用 K-means++（选择距离现有中心较远的点）。
* 手肘法（Elbow Method）、轮廓系数（Silhouette Score）。


### 层次聚类（Hierarchical Clustering）

问题：
* 层次聚类的两种方式（聚合式 / 分裂式）区别？
* 如何选择距离度量（如欧氏距离、曼哈顿距离）？

答案：
* 聚合式自底向上合并簇，分裂式自顶向下拆分。
* 根据数据类型选择：连续型数据常用欧氏距离，文本数据常用余弦相似度。

### 回归模型-线性回归（Linear Regression）

问题：
* 线性回归的假设条件有哪些？
* 如何处理多重共线性？
* 评估指标有哪些？

答案：
* 线性关系、独立同分布、无自相关性、残差正态分布。
* 正则化（L1/L2）、特征选择、主成分分析（PCA）。
* MSE（均方误差）、R² 分数、Adjusted R²。


### 多项式回归（Polynomial Regression）

问题：
* 多项式回归如何处理非线性关系？
* 如何避免高次多项式过拟合？

答案：
* 添加多项式特征（如 x², x³），将线性模型扩展为非线性。
* 正则化（L2）、交叉验证选择最优多项式阶数

### 模型调参

问题：
* 网格搜索（Grid Search）和随机搜索（Random Search）的区别？
* 贝叶斯优化（Bayesian Optimization）的优势？

答案：
* 网格搜索穷举所有参数组合，随机搜索随机采样，后者更高效。
* 利用历史采样结果动态调整搜索范围，适合高维参数空间。


8. 数据不平衡

问题：
* 分类任务中数据不平衡的解决方法？
* 回归任务中异常值的处理？

答案：
* 过采样（SMOTE）、欠采样、调整类别权重、使用 F1 分数评估。
* 识别并删除异常值、使用稳健回归（如 Huber Loss）。


### 线性判别分析（LDA）
* 核心思想：寻找投影方向，使类间方差最大化、类内方差最小化，用于降维和分类。
* 假设条件：
    * 各特征服从高斯分布。
    * 类内协方差矩阵相同。
* 优点：
    * 监督降维，保留类别信息。
    * 适合线性可分数据，计算高效。
* 缺点：
    * 对非线性数据效果差。
    * 需满足严格假设条件。
* 应用场景：
    * 降维（如人脸识别预处理）。
    * 二分类或多分类任务（如垃圾邮件分类）。

问题：LDA 和 PCA 的区别？

答案：
* LDA 是监督降维，利用类别信息；PCA 是无监督降维。
* LDA 最大化类间差异，PCA 最大化数据方差。
* LDA 降维后维度不超过类别数 - 1，PCA 无此限制。

问题：LDA 的假设条件有哪些？

答案：
* 特征服从高斯分布。
* 类内协方差矩阵相同。
* 样本独立同分布

LDA 和 QDA（二次判别分析）的区别？

（答案：QDA 允许不同类协方差矩阵，适用于非线性数据，但易过拟合。）

### 支持向量机（SVM）
* 核心思想：寻找最大间隔的超平面，通过核函数处理非线性问题。
* 关键概念：
    * 核函数：将低维数据映射到高维空间（如 RBF 核、多项式核）。
    * 软间隔：允许部分样本出错，通过惩罚参数C平衡准确率和间隔。
* 优点：
    * 对小样本高维数据效果好。
    * 理论基础扎实，泛化能力强。
* 缺点：
    * 计算复杂度高（尤其核函数）。
    * 对参数敏感（需网格搜索调参）。
* 应用场景：
    * 文本分类（如情感分析）。
    * 图像识别（如手写数字识别）
* SVM 的对偶问题是什么？为什么需要对偶形式？
    * （答案：将原始优化问题转换为对偶问题，便于引入核函数并处理高维数据。）。

问题：SVM 为什么适合小样本高维数据？

答案：
* 核函数能高效处理高维空间，避免维度灾难。
* 最大间隔准则对噪声鲁棒性强。

问题：如何选择 SVM 的核函数？

答案：
* 线性核：数据线性可分（如文本分类）。
* RBF 核：通用选择，适合非线性数据。
* 多项式核：计算复杂，易过拟合。

问题：SVM 如何处理不平衡数据？

答案：
* 调整class_weight参数。
* 使用 SMOTE 过采样少数类。
* 采用代价敏感学习（如调整误分类成本）