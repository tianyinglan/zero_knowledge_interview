 # cuda
 ## CUDA 多核管理技术
1. 线程层次结构
    * 网格（Grid）：包含多个线程块（Block），每个 Block 最多 1024 线程。
    * 线程束（Warp）：32 个线程组成一个 Warp，SIMT 架构下所有线程执行相同指令。
    * 协作组（Cooperative Groups）：支持跨 Block 同步，用于处理全局数据依赖。
2. 内存管理策略
    * 三级缓存架构：
        * 寄存器（Registers）：每个线程 64KB，访问延迟 0.5-1 个周期。
        * 共享内存（Shared Memory）：每个 Block 96KB，延迟约 10 个周期。
        * 全局内存（Global Memory）：延迟约 400-600 个周期，通过合并访问优化。
    * 内存合并（Memory Coalescing）：相邻线程访问连续地址，提升带宽利用率。例如，在 1D-FFT 中，线程索引 i 与地址偏移 i 对齐。
3. 任务调度与同步
    * 动态并行：子核函数可递归启动新线程，如 cuFFT 的递归实现。
    * 事件（Event）同步：使用 cudaEvent_t 标记计算阶段，精确控制数据流。
    * 流（Stream）管理：多个流并行处理不同任务，实现计算与数据传输重叠。
4. 性能优化技术
    * 寄存器分配：限制每个线程使用寄存器数（如 < 64KB），避免溢出到本地内存。
    * 共享内存 bank 冲突：通过填充（Padding）解决连续地址访问冲突。例如，复数数据按 2 倍间隔存储。
    * 循环展开：手动展开蝶形运算循环，减少分支预测失败。

1. 线程层次结构
    * 描述 Grid/Block/Thread 三级结构，说明每个维度最大尺寸限制（如 SM8.0 的 1024 threads/block）
    * Grid（网格）
    * Grid 是最高级别的线程组织单位，它代表了整个并行计算任务。一个 Grid 由多个 Block 组成，这些 Block 可以分布在多个流式多处理器（SM）上并行执行。可以把 Grid 看作是一个二维或三维的数组，其中每个元素是一个 Block。在实际应用中，Grid 通常用于处理大规模的数据，将大任务分割成多个小的子任务，每个子任务由一个 Block 来处理。
    * Block（线程块）
    * Block 是 Grid 中的一个子单元，由多个 Thread 组成。同一个 Block 内的线程可以通过共享内存进行通信和同步，这使得它们能够高效地协作完成一个子任务。Block 也可以是二维或三维的，这种多维结构有助于处理多维数据，例如二维图像或三维体数据。不同 Block 之间是相互独立的，可以并行执行，从而实现大规模的并行计算。
    * Thread（线程）
    * Thread 是最小的执行单元，负责执行具体的计算任务。每个 Thread 都有自己的线程 ID，可以根据这个 ID 来确定它在 Block 和 Grid 中的位置，进而访问相应的数据。多个 Thread 可以并行执行相同的代码，从而实现数据并行。

如何通过dim3定义并行度
1. 内存模型
* 对比 Global/Shared/Register/Constant 纹理内存的访问速度与适用场景
    * Register（寄存器） > Shared Memory（共享内存） > Constant Memory（常量内存）> Texture Memory（纹理内存）> Global Memory（全局内存）
* 如何减少 Global Memory 访问延迟？（如合并访问、预取、分块）
    * 预取可以通过异步内存复制（cudaMemcpyAsync）来实现，将数据传输和计算操作重叠进行。

1. 同步机制
    * __syncthreads()的作用域与注意事项
        * __syncthreads() 的作用域仅限于调用它的线程块，线程块内所有线程必须到达同步点，不能在不同线程块之间同步，避免在循环中过度使用， 共享内存访问的一致性

* 解释 warp 中的线程分支执行机制（Branch Divergence）
    * 在 NVIDIA GPU 架构里，多个线程会被组织成一个 Warp，通常一个 Warp 包含 32 个线程。这些线程在硬件层面会以 SIMT（单指令多线程）的方式执行，即同一个 Warp 内的所有线程会同时执行相同的指令。不过，每个线程会依据自身的线程 ID 来处理不同的数据。

* 数据重排：在数据处理之前，对数据进行重排，使得 Warp 内的线程对分支条件的判断结果尽量一致。例如，将偶数数据和奇数数据分别存储在一起，这样在执行分支语句时，Warp 内的线程就可以避免分支分歧。
* 减少分支语句：尽量减少代码中的分支语句，或者将分支语句提前到 CPU 端进行处理。例如，可以在 CPU 端根据数据的特点预先计算好每个线程的执行路径，然后将结果传递给 GPU，这样 GPU 就可以避免执行分支语句。
* 使用掩码操作：对于一些简单的分支判断，可以使用掩码操作来替代分支语句。

4. 内存访问模式
    * 如何避免 Shared Memory Bank 冲突？ 32 个 Bank，当多个线程访问同一 Bank 的不同地址时会引发 Bank 冲突，导致访问串行化（如填充、转置， 调整线程索引，偶数 / 奇数 Bank 交替访问，循环展开，避免跨 Bank 的连续访问）
* 给出合并访问的代码示例（如访问data[i][j]时 i 为线程索引）
        * 合并访问是指让相邻线程访问全局内存中连续的地址，这样可以将多个线程的内存访问请求合并成一个或少数几个内存事务，从而减少内存控制器的开销，提高内存带宽利用率

1. 数据划分策略
    * 如何将二维矩阵划分为 Tile？（如 32x32 子块），有助于提高数据局部性和减少全局内存访问次数，尤其在矩阵乘法等计算密集型任务中非常有效
    * 1	确定矩阵大小：明确矩阵的行数和列数。
        * 计算 Tile 数量：根据矩阵大小和 Tile 的尺寸（32x32），计算矩阵可以划分成多少个 Tile。
        * 线程块和线程网格的组织：使用线程块和线程网格来处理每个 Tile。每个线程块负责一个 Tile 的计算，线程块内的线程负责 Tile 内的具体元素计算。
        * 数据加载和计算：将 Tile 的数据从全局内存加载到共享内存中，然后在共享内存上进行计算，以减少全局内存访问延迟。

* 对比行优先与列优先存储对性能的影响
    * 全局内存访问效率
        * 行优先存储
            * 优势场景：当按行访问矩阵元素时，行优先存储具有明显优势。因为相邻元素在内存中是连续存储的，符合合并访问的条件，能将多个线程的内存访问请求合并成一个或少数几个内存事务，减少内存控制器的开销，提高内存带宽利用率。例如在矩阵乘法中，如果一个线程块处理矩阵的一行，使用行优先存储可以实现高效的合并访问。
        * 列优先存储
            * 优势场景：若按列访问矩阵元素，列优先存储更合适。此时相邻列元素在内存中连续，能实现合并访问。比如在某些算法中需要按列对矩阵进行求和、求平均等操作，列优先存储可提高内存访问效率。
    * 缓存命中率
        * 行优先存储：在大多数情况下，程序中的循环通常是按行遍历矩阵，行优先存储使得相邻元素在内存中连续，更容易被缓存命中。因为缓存是按块进行数据加载的，当一个元素被访问时，其相邻的元素也会被加载到缓存中，后续对这些相邻元素的访问就可以直接从缓存中获取，减少了从全局内存加载数据的时间。
        * 列优先存储：如果程序按列遍历矩阵，列优先存储能提高缓存命中率。但如果循环是按行编写的，采用列优先存储会导致缓存命中率降低，因为按行访问时，相邻元素在内存中不连续，每次访问可能都需要从全局内存加载新的数据块到缓存中
    * 共享内存使用
        * 行优先存储：在使用共享内存优化矩阵计算时，如果按行将矩阵的一部分加载到共享内存中，行优先存储可以方便地实现连续加载，减少共享内存的访问冲突。例如在矩阵乘法中，将矩阵的一行加载到共享内存后，线程可以高效地对其进行操作。
        * 列优先存储：当需要按列将矩阵元素加载到共享内存时，列优先存储更便于实现连续加载。但如果算法设计是基于行操作的，使用列优先存储可能会增加共享内存的管理复杂度

1. 流水线处理
    * 如何通过流（Stream）实现计算与数据传输的重叠？
        * 它允许不同的操作（如数据传输、内核计算）异步执行，从而实现计算与数据传输的重叠。提高 GPU 的利用率和程序的整体性能
            * 创建流：使用 cudaStreamCreate 函数创建一个或多个流。
            * 异步数据传输：使用 cudaMemcpyAsync 函数代替 cudaMemcpy 进行数据传输，这样数据传输操作会异步执行，不会阻塞后续操作。
            * 异步内核调用：在调用内核函数时，指定流作为参数，使内核计算也能异步执行。
            * 同步流：使用 cudaStreamSynchronize 函数确保流中的所有操作都已完成。

矩阵乘法优化
* 如何用 Shared Memory 优化 GEMM？（包括 Tile 大小选择、寄存器重用）
    * Tile大小选择
        * 共享内存容量：Tile 的大小不能超过共享内存的容量，否则会导致共享内存溢出。
        * 线程块大小：Tile 的大小应与线程块的大小相匹配，以充分利用线程块内的线程并行性。
        * 数据访问模式：Tile 的大小应使得数据访问具有良好的合并性，减少内存访问冲突。

* 解释 CUTLASS 库中的分块策略（如cutlass::gemm::GemmCoord）
    * 矩阵分块：将输入矩阵 A、B和输出矩阵 C 分别划分为多个小的块。这些块可以存储在共享内存或寄存器中，以提高数据访问速度。
    * 分块计算：按块对矩阵进行计算，每次只处理一个或几个块的数据。在计算过程中，将块数据从全局内存加载到共享内存或寄存器中，进行计算后再将结果写回全局内存。
    * 循环分块：通过循环的方式依次处理所有的块，直到完成整个矩阵乘法。

* 卷积计算
* 如何用 CUDA 实现高效卷积？（分块策略、边界处理）
    * 分块策略是提高卷积计算效率的核心，其基本思想是将大的卷积任务划分为多个小的子任务，利用共享内存减少全局内存访问次数，提高数据局部性。以下是分块策略的具体步骤：
        * 定义分块大小：根据硬件特性和问题规模，确定合适的分块大小。通常，分块大小应与线程块的大小相匹配，以充分利用线程并行性。
        * 数据加载：将输入特征图和卷积核的相关数据从全局内存加载到共享内存中。每个线程块负责加载一部分数据，确保数据的合并访问。
        * 卷积计算：在共享内存中进行卷积计算，每个线程负责计算输出特征图的一个或多个元素。
        * 数据写入：将计算结果从共享内存写回到全局内存中。

* 在卷积计算中，边界处理是一个重要的问题。当卷积核滑过输入特征图的边界时，需要确保不会访问越界的内存地址。常见的边界处理方法有零填充（Zero Padding）和边界复制（Boundary Replication）。这里以零填充为例进行说明
    * 对比 im2col 与 Winograd 算法的优缺点
    * im2col 算法：该算法的核心思想是把卷积操作转化为矩阵乘法。它将输入图像的局部区域（卷积核滑动的区域）展开成列向量，将卷积核也展开成矩阵，这样卷积操作就可以通过矩阵乘法来实现。由于矩阵乘法有很多高效的库（如 BLAS）可以利用，所以可以借助这些库来加速卷积计算。
    * Winograd 算法：Winograd 算法是一种基于数学变换的算法，它利用特殊的变换矩阵把卷积计算转化为在变换域中的少量乘法和大量加法运算。通过减少乘法的数量，从而减少计算量，达到加速卷积的目的

1. 常见问题诊断
    * 解释unsynchronized thread exit错误的可能原因
        * 1. 条件分支导致线程提前退出
        * 2. 异常情况导致线程异常退出
        * 3. 递归调用导致线程退出不一致
        * 4. 同步点位置不当

* 统一内存（Unified Memory）
    * 自动内存管理的工作原理，何时适合使用？
        1. 引用计数（Reference Counting）
        2. 垃圾回收（Garbage Collection，GC）

* 如何通过cudaMemPrefetchAsync()优化数据分布
   * 在 GPU 计算中，数据在不同内存之间的传输（如从主机内存到设备内存）通常是一个瓶颈。cudaMemPrefetchAsync() 允许在 GPU 执行计算任务的同时，异步地将数据预取到目标内存位置。这样，当 GPU 需要访问这些数据时，数据已经在目标内存中，从而减少了等待数据传输的时间。

1. 多 GPU 编程
* 对比单 GPU 与多 GPU 并行的挑战（通信开销、负载均衡）
    * 单 GPU
        * 情况说明：单 GPU 系统中，数据主要在设备内的不同存储层次（如全局内存、共享内存、寄存器）之间流动。由于这些存储层次都在同一 GPU 芯片内部，数据传输距离短，硬件设计上针对这种内部通信进行了优化，所以通信延迟低，带宽相对较高。
        * 举例：在单 GPU 上进行矩阵乘法运算时，数据从全局内存加载到共享内存，再从共享内存到寄存器进行计算，整个过程在 GPU 内部完成，数据传输速度快，通信开销可忽略不计
    * 多 GPU
        * 情况说明：多 GPU 系统中，不同 GPU 之间需要进行数据通信。数据在不同 GPU 的全局内存之间传输时，需要通过 PCIe 总线或者 NVLink 等外部连接方式，这些连接的带宽和延迟特性远不如单 GPU 内部的通信。而且，随着 GPU 数量的增加，通信的复杂度也会上升，可能出现数据竞争和拥塞问题。
        * 举例：在多 GPU 并行的卷积神经网络训练中，不同 GPU 负责不同批次的数据计算，在梯度更新阶段，需要将各个 GPU 计算得到的梯度汇总，这个过程涉及大量的数据在不同 GPU 之间传输，通信开销显著增加。若采用 PCIe 总线连接，其带宽有限，可能成为性能瓶颈；即便使用 NVLink，在高并发通信时也可能出现拥塞。
* 负载均衡
    * 单 GPU
        * 情况说明：单 GPU 的负载均衡相对简单，因为所有计算任务都在一个设备上执行。开发者只需关注如何将任务合理地分配到 GPU 的各个线程块和线程中，以充分利用 GPU 的并行计算资源。通常可以通过调整线程块和网格的维度来实现一定程度的负载均衡。
        * 举例：对于一个图像处理任务，单 GPU 可以将图像分块，每个线程块处理一个图像块，通过合理设置线程块的数量和大小，使每个线程块的计算量大致相等，从而实现负载均衡。
    * 多 GPU
        * 情况说明：多 GPU 并行中，负载均衡是一个复杂的问题。不同 GPU 的性能可能存在差异，而且任务的划分和分配也更为困难。如果任务分配不均匀，会导致部分 GPU 闲置，而部分 GPU 过载，从而降低整体性能。此外，动态变化的任务也增加了负载均衡的难度，需要实时调整任务分配策略。
        * 举例：在多 GPU 并行的科学计算中，不同的计算任务可能具有不同的复杂度和计算量。如果简单地将任务平均分配给各个 GPU，可能会出现某些 GPU 很快完成任务，而其他 GPU 还在忙碌的情况。例如，在计算流体力学模拟中，不同区域的流体流动复杂度不同，若不根据实际情况调整任务分配，就会造成负载不均衡


* 给出使用cudaSetDevice()切换设备的代码示例
1. 异步计算Asynchronous Computation
* 如何用cudaStreamWaitEvent()实现事件同步？
* 1. 创建事件和流
    * 使用 cudaEventCreate() 创建事件，使用 cudaStreamCreate() 创建流。
* 2. 记录事件 Record an Event
    * 在某个流中使用 cudaEventRecord() 记录事件，这意味着从该时刻起事件开始进行。
* 3. 让流等待事件Make a Stream Wait for the Event
    * 调用 cudaStreamWaitEvent() 让另一个流等待这个事件完成。
Call cudaStreamWaitEvent() to make another stream wait for the completion of this event.

* 4. 同步和清理 Synchronize and Clean Up
    * 使用 cudaEventSynchronize() 确保事件完成，最后使用 cudaEventDestroy() 和 cudaStreamDestroy() 销毁事件和流。

* 解释cudaLaunchCooperativeKernel()的协作启动机制
* Purpose: Enables kernels on different devices (GPUs) to collaborate during execution, reducing synchronization overhead and improving parallel efficiency.
    * Key Features:
        * Cross-Device Collaboration:
            * Allows multiple GPUs to execute a single kernel as a cohesive unit.
            * Coordinates memory access and synchronization across devices.
        * Reduced Overhead:
            * Eliminates explicit synchronization steps between separate kernel launches.
            * Optimizes data transfer and computation overlap.
        * Use Cases:
            * Large-scale parallel algorithms requiring global data dependencies.
            * Multi-GPU neural network training with inter-GPU communication
15. 实现向量加法
    * 编写 CUDA 内核实现C = A + B，包含错误检查
    * #include <iostream>
    * #include <cuda_runtime.h>
    * 
    * // 定义一个检查 CUDA 调用错误的宏
    * #define CUDA_CHECK(call) { \
    *     cudaError_t err = call; \
    *     if (err != cudaSuccess) { \
    *         std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__ << " code=" << err << " (" << cudaGetErrorString(err) << ") \n"; \
    *         exit(EXIT_FAILURE); \
    *     } \
    * }
    * 
    * // CUDA 内核函数，实现向量加法 C = A + B
    * __global__ void vectorAdd(const float *A, const float *B, float *C, int n) {
    *     int idx = threadIdx.x + blockIdx.x * blockDim.x;
    *     if (idx < n) {
    *         C[idx] = A[idx] + B[idx];
    *     }
    * }
    * 
    * int main() {
    *     const int n = 1000;
    *     const int size = n * sizeof(float);
    * 
    *     // 主机端数组
    *     float *h_A = new float[n];
    *     float *h_B = new float[n];
    *     float *h_C = new float[n];
    * 
    *     // 初始化主机端数组
    *     for (int i = 0; i < n; ++i) {
    *         h_A[i] = static_cast<float>(i);
    *         h_B[i] = static_cast<float>(i * 2);
    *     }
    * 
    *     // 设备端数组
    *     float *d_A, *d_B, *d_C;
    * 
    *     // 分配设备端内存
    *     CUDA_CHECK(cudaMalloc((void**)&d_A, size));
    *     CUDA_CHECK(cudaMalloc((void**)&d_B, size));
    *     CUDA_CHECK(cudaMalloc((void**)&d_C, size));
    * 
    *     // 将数据从主机复制到设备
    *     CUDA_CHECK(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice));
    *     CUDA_CHECK(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice));
    * 
    *     // 定义线程块和网格的维度
    *     dim3 block(256);
    *     dim3 grid((n + block.x - 1) / block.x);
    * 
    *     // 启动内核
    *     vectorAdd<<<grid, block>>>(d_A, d_B, d_C);
    * 
    *     // 检查内核启动是否出错
    *     cudaError_t err = cudaGetLastError();
    *     if (err != cudaSuccess) {
    *         std::cerr << "CUDA kernel launch failed: " << cudaGetErrorString(err) << std::endl;
    *         return EXIT_FAILURE;
    *     }
    * 
    *     // 同步设备，确保内核执行完毕
    *     CUDA_CHECK(cudaDeviceSynchronize());
    * 
    *     // 将结果从设备复制回主机
    *     CUDA_CHECK(cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost));
    * 
    *     // 输出部分结果进行验证
    *     for (int i = 0; i < 5; ++i) {
    *         std::cout << "C[" << i << "] = " << h_C[i] << std::endl;
    *     }
    * 
    *     // 释放设备端内存
    *     CUDA_CHECK(cudaFree(d_A));
    *     CUDA_CHECK(cudaFree(d_B));
    *     CUDA_CHECK(cudaFree(d_C));
    * 
    *     // 释放主机端内存
    *     delete[] h_A;
    *     delete[] h_B;
    *     delete[] h_C;
    * 
    *     return 0;
    * }    
    * 优化版本：使用共享内存减少全局内存访问次数
16. 矩阵转置
    * 实现基于共享内存的转置内核，避免 Bank 冲突
    * 处理非对齐尺寸（如 33x33 矩阵）的方法
        * 内存访问优化：合并访问（连续地址）、避免 Bank 冲突（填充至 32 字节倍数）、预取到 Shared Memory
        * 性能瓶颈定位：使用 Nsight Compute 查看 Occupancy、Memory Throughput、SM 利用率
        * 并行度设计：每个 SM 至少需要 24 个 warps 以隐藏内存延迟（根据架构调整）
        * 混合精度计算：使用__half2类型加速，注意数值稳定性（如 FP16 累加后用 FP32 归约）