# msm
## Optimization of Multi-Scalar Multiplication (MSM)
1. Pippenger Algorithm
    * Principle: Decompose scalars into fixed-window blocks (e.g., 8-bit). Reduce the number of point multiplications (PMULT) through grouped accumulation.
        * Scalar decomposition: Decompose the scalar $k_i$ into blocks $b_i[j]$, and group points into different buckets according to block values.
        * Grouped accumulation: Accumulate points in each bucket (PADD), and finally merge results through weight.
    * Advantages:
        * Reduce the complexity from O(n⋅PMULT) to O(number of buckets⋅PADD), decreasing the computation amount.
        * Utilize scalar sparsity (such as 0 and 1) to further reduce the computational load.
2. Dynamic Work Scheduling Mechanism
    * Shared addition module: All buckets share a pipelined adder to avoid resource waste.
    * FIFO buffering: Use multiple FIFO queues to temporarily store points awaiting processing, enabling dynamic task allocation.
    * Parallel Processing Element (PE): Each PE independently processes scalars of different blocks, supporting parallel computation.
    * Load balancing:
        * Each PE processes fixed-digit blocks (e.g., 4-bit) and handles multiple groups of blocks in parallel.
        * Alleviate scalar sparsity through grouping to ensure balanced load for each PE.
    * Pipelined Adder Design
        * 74-stage Pipeline: Supports efficient modular arithmetic (such as Montgomery multiplication), handling the addition of two points per cycle.
        * Parallel Processing Flow: Each PE (Processing Element) independently processes a group of blocks, and results are transmitted to the shared addition module via FIFO.
3. Hardware Architecture Optimization
    1. Bandwidth-Efficient NTT Module
        * Pipelined Design: Achieve step-size access via FIFO, reducing resource overhead of multiplexers.
        * Reconfigurability: Support NTT cores of different sizes to adapt to dynamic demands.
    2. On-Chip Memory Management
        * Global Buffer: Temporarily store scalar and point data to reduce off-chip access times.
        * Data Alignment: Read data in blocks to enhance memory access efficiency.
    3. Parallel Processing Element (PE)
        * Independent Processing: Each PE processes scalars of different blocks, supporting parallel computation.
        * Resource Sharing: Share adders and FIFO to reduce hardware costs.
4. Other Optimization Strategies
    * Special Value Handling
        * When the scalar is 0 or 1, directly skip PMULT to reduce ineffective calculations.
    * Collaborative Optimization of Polynomials and MSM
        * After the scalar Hn undergoes NTT, it exhibits a uniform distribution, reducing the probability of the worst-case scenario.
    * Heterogeneous Computing
        * The G2 curve part of MSM is processed by the CPU to balance resource utilization.

## MSM (Multi-Scalar Multiplication) Technical Solutions and Optimization Ideas
1. Technical Solutions
    * Algorithm Principle:Based on the Pippenger algorithm, decompose 253-bit scalars into multiple fixed windows (12–13 bits), reducing the number of point multiplications (PMULT) through grouped accumulation. Specific steps are as follows:
        * Scalar Decomposition: Decompose scalars into blocks according to the window size.
        * Grouped Accumulation: Group points into different buckets (Bucket) by block values. After accumulating points in the bucket, multiply by the block value and merge.
        * Weight Merging: Finally, merge results through the weight 2j⋅s of each block.
    * Hardware Implementation:
        * FPGA Architecture: Implemented on AWS F1 instances, supporting the BLS12-377 G1 curve.
        * Pipelined Adder: Full pipelined design, handling two-point addition per cycle with a latency of 238 cycles.
        * Dynamic Scheduling: Share adders, and use FIFO buffering and multiple processing units (PE) to handle different windows in parallel.
2. Optimization Ideas
    * Reduce PMULT Times:
        * Through grouped accumulation, reduce the number of PMULT from O(n⋅λ) to O(number of buckets⋅PADD), and further optimize by leveraging scalar sparsity (0/1 values).
    * Resource Sharing:
        * All buckets share one pipelined adder to reduce hardware costs.
    * Efficient Memory Access:
        * Preprocess point data into extended coordinates of the twisted Edwards curve to reduce modular operation complexity.
        * Store bucket data in on-chip URAM, supporting non-uniform bucket sizes.
    * Parallel Processing:
        * Multiple PEs process different windows in parallel to mask PCIe latency and host post-processing time.
    * Special Value Handling:
        * Skip PMULT directly when the scalar is 0 or 1 to reduce ineffective calculations.

### Performance and Resource Comparison

| Module | Optimization Method  | Performance | Characteristics |
| ---- | ---- | ---- | ---- |
| MSM | Pippenger Algorithm + Dynamic Scheduling | 13.200 Mop/s（226 点） | Shared adder, low resource consumption |
| NTT | Recursive Decomposition + Memory Optimization | 224 点 NTT 延迟 0.045 秒（64 核） | High parallelism, improved HBM bandwidth utilization |



### Summary
* Through algorithm optimization and hardware sharing, MSM significantly reduces computation and improves throughput, suitable for FPGA environments with limited resources.
* Through recursive decomposition and memory scheduling, NTT balances computational complexity and hardware resources, achieving efficient parallel processing.
* Both achieve low-latency and high-throughput computational acceleration in scenarios like zero-knowledge proofs through hardware-software co-design.

## Montgomery Algorithm
* Montgomery Algorithm is an efficient method for computing the modular multiplication of large integers, widely applied in cryptography (such as RSA, ECC, homomorphic encryption, etc.). 
* Its core idea lies in transforming modular multiplication into unsigned integer operations, avoiding direct computation of large integer division, thereby improving computational efficiency.