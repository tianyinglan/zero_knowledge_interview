# machine_learning
1. Supervised Learning vs Unsupervised Learning

Question: Distinguish between supervised learning and unsupervised learning, and give an application scenario for each.

Answer:
* Supervised learning: Trains a model using labeled data with the goal of predicting outputs (such as classification and regression).
    * Application: Image classification (e.g., ResNet for identifying cats and dogs).
* Unsupervised learning: Discovers patterns from unlabeled data (such as clustering and dimensionality reduction).
    * Application: Customer segmentation (K - means for partitioning user behavior).

2. Bias - Variance Trade - off

Question: Explain the bias - variance trade - off and describe how to solve the problems of high bias and high variance.

Answer:
* Bias: The model underfits and fails to capture the true patterns in the data (e.g., using a linear model to handle non - linear relationships).
* Variance: The model overfits and is sensitive to noise (e.g., fitting a small amount of data with a high - order polynomial).

Solutions:
* High bias: Increase the model complexity (e.g., use a neural network instead of linear regression).
* High variance: Regularization (L2 regularization), increase the data volume, and use ensemble learning (random forest).  

3. Overfitting and Underfitting

Question: How to determine if a model is overfitting? List three methods to prevent overfitting.

Answer:
* Signs of overfitting: High accuracy on the training set but low accuracy on the validation set.
* Preventive methods:
    * Regularization (e.g., L1/L2 regularization).
    * Early stopping.
    * Data augmentation (e.g., image rotation and translation). 

4. Cross - Validation

Question: Explain the principle and advantages of k - fold cross - validation.

Answer:
* Principle: Divide the dataset into k subsets. Each time, use k - 1 subsets for training and 1 subset for validation, and repeat this process k times and take the average.

Advantages:
* More accurately evaluate the model's generalization ability.
* Effectively utilize the data (each sample is trained k - 1 times).

5. Activation Functions

Question: Compare the advantages and disadvantages of the Sigmoid and ReLU activation functions.

Answer:
| Function | Advantages                           | Disadvantages                                   |
|----------|--------------------------------|----------------------------------------|
| Sigmoid  | 	Output range is from 0 to 1, suitable for classification        | Vanishing gradient, non - zero - centered output               |
| ReLU     | Fast computation, alleviates the vanishing gradient problem          | Dead ReLU problem (some neurons are never activated）   |	

Leaky ReLU: Allows a small gradient for negative inputs (e.g., f(x) = max(0.1x, x)).

6. Gradient Descent Optimizers

Question: Explain the principle of the Adam optimizer and the roles of its hyperparameters β₁ and β₂.

Answer:
Principle: Combines momentum and RMSprop to adaptively adjust the learning rate.

Hyperparameters:
* β₁ (default 0.9): The exponential decay rate for the first - moment estimate.
* β₂ (default 0.999): The exponential decay rate for the second - moment estimate.

7. Feature Engineering

Question: List three common feature engineering methods.

Answer:
* Standardization/normalization: Make the features have a mean of 0 and a variance of 1 (e.g., StandardScaler).
* One - hot encoding: Convert categorical variables into binary vectors (e.g., OneHotEncoder).
* Feature interaction: Create new features (e.g., age², income × education).

8. Question: Explain the differences between Bagging and Boosting.
Answer:

| Method     | Core Idea                                                                 | Typical Algorithms       | Parallelism       |
|------------|--------------------------------------------------------------------------|--------------------------|--------------------|
| Bagging    | Bootstrap sampling to train base models, averaging results (reduces variance) | Random Forest            | Supports parallelism |
| Boosting   | Iteratively trains weak models, adjusts sample weights (reduces bias)    | XGBoost, LightGBM        | Sequential execution |

9.Evaluation Metrics

Question: For imbalanced data, why is accuracy not the best metric? What alternative metrics should be used?

Answer:
* Reason: Accuracy cannot reflect the true performance of the model when the ratio of positive and negative samples is extremely unbalanced (e.g., when 99% of the samples are negative, predicting all samples as negative gives an accuracy of 99%, but it is meaningless).
* Alternative metrics:
    * Precision - Recall Curve.
    * F1 - Score.
    * AUC - ROC Curve (insensitive to class imbalance).

10. Deep Learning Frameworks
Question: Compare the advantages and disadvantages of PyTorch and TensorFlow.
Answer:

| Framework | Advantages                                                                 | Disadvantages               |
|-----------|-----------------------------------------------------------------------------|-----------------------------|
| PyTorch   | Dynamic computational graph (ease of debugging), high flexibility           | Weak support for deployment |
| TensorFlow| Static computational graph (optimized for performance), well-developed ecosystem | Lower flexibility         |

Application scenarios:		

* PyTorch: Research (rapid iteration).
* TensorFlow: Industrial deployment (model export, mobile support).


1. Transfer learning: Why do pre - trained models perform better on small datasets?
2. Generative Adversarial Networks (GAN): Explain the game process between the generator and the discriminator.
3. Transformer model: What are the differences between the attention mechanism and Recurrent Neural Networks (RNN)?

Answers:
1. Pre - trained models have learned general features. For small datasets, only the last few layers need to be fine - tuned to adapt to new tasks.
2. Generator: Generates fake samples to deceive the discriminator; Discriminator: Distinguishes between real and fake samples. The two are optimized alternately.
3. Transformer: Captures long - range dependencies through self - attention and has no recurrent structure; RNN: Processes sequences step by step in time and is prone to the vanishing gradient problem.


### Decision Tree
Question:
* How does a decision tree select the optimal splitting feature?
* What is the difference between Information Gain and Gini Index?
* How to prevent decision tree overfitting?
Answer:
* Optimal features are selected by calculating metrics like Information Gain, Gain Ratio, or Gini Index.
* Information Gain measures entropy reduction, while Gini Index quantifies impurity. The latter is computationally more efficient.
* Techniques include pruning (pre/post), limiting tree depth, and controlling minimum leaf samples. Overfitting arises because greedy algorithms pursue local optimality, leading to excessively deep trees.

Core Idea: Recursively partition samples into purer subsets via feature splits, forming a hierarchical tree.
* Feature Selection Criteria:
    * Classification Trees: Information Gain (ID3), Gain Ratio (C4.5), Gini Impurity (CART).
    * Regression Trees: Mean Squared Error (CART).



## Random Forest
* Core Idea: An ensemble method using Bagging to train multiple decision trees and aggregate predictions via voting/averaging.

Key Techniques:
* Bootstrap Sampling: Randomly sample training subsets to reduce overfitting.
* Random Feature Subsets: Select a random subset of features for splitting, enhancing diversity.

Disadvantages:
* Sensitive to noise (especially with deep trees).
* Slower prediction speed (requires traversing all trees).

Question:
* How does Random Forest reduce variance?
* What are the roles of Bootstrap and feature subsets?
* Difference between Random Forest and Gradient Boosting (e.g., XGBoost)?

Answer:
* By averaging predictions across diverse trees trained on Bootstrap samples and random features.
* Bootstrap reduces overfitting, and feature subsets increase model diversity.
* Random Forest: Parallel Bagging reduces variance. XGBoost: Sequential Boosting reduces bias via gradient optimization.

Question: How to select n_estimators and max_depth?

Answer:
* n_estimators: Larger values reduce variance but increase computational cost. Choose via cross-validation.
* max_depth: Balance bias-variance trade-off (e.g., set to 5-10 for most cases).

Question: How is feature importance calculated in Random Forest?

Answer: By summing the impurity reduction contributed by each feature across all trees.

## XGBoost (Extreme Gradient Boosting)

Question: What improvements does XGBoost offer over traditional GBDT? What are the roles of key hyperparameters?
Answer:
Improvements:
Second-order Taylor expansion for faster convergence.
Regularization (L1/L2) to prevent overfitting.
Parallel computation of feature splits for acceleration.
Key Hyperparameters:
n_estimators: Number of trees (reduces variance but increases cost).
max_depth: Tree depth (balance overfitting).
learning_rate: Step size (controls model complexity).
subsample: Sample fraction (reduces variance).
Best Practices:
Use model.feature_importances_ to analyze feature contributions.
Tune parameters with GridSearchCV or XGBoost’s built-in cv.
Visualize trees with graphviz for interpretability.

| Model       | Advantages                              | Disadvantages                      | Application Scenarios              |
|-------------|-----------------------------------------|------------------------------------|------------------------------------|
| Decision Tree | High interpretability, no feature scaling required | Prone to overfitting, poor generalization | Rapid prototyping                  |
| Random Forest | Strong anti-overfitting, parallel training | High computational resource consumption | Large-scale data classification/regression |
| XGBoost     | High accuracy, efficient handling of missing/sparse data | Complex hyperparameter tuning | Competitions and industrial tasks |

### K-means
* What are the core steps of K-means?
* How to select initial centroids?
* How to determine the optimal number of clusters K?

Answer:
* Initialize centroids → Assign samples → Update centroids → Repeat until convergence.
* Use k-means++ (distance-based initialization) instead of random selection.
* Elbow Method (inertia plot) or Silhouette Score.

### Hierarchical Clustering
* What is the difference between agglomerative and divisive hierarchical clustering?
* How to choose distance metrics (e.g., Euclidean vs. Manhattan)?

Answer:
* Agglomerative: Merge clusters bottom-up. Divisive: Split clusters top-down.
* Select based on data type: Use Euclidean for continuous data, cosine similarity for text.

### Linear Regression
* What are the assumptions of linear regression?
* How to handle multicollinearity?
* What are the evaluation metrics?

Answer:
* Linear relationship, independent errors, no autocorrelation, normal residuals.
* Regularization (L1/L2), feature selection, or PCA.
* MSE, R², Adjusted R².

### Polynomial Regression
* How does polynomial regression handle nonlinear relationships?
* How to avoid overfitting with high-degree polynomials?

Answer:
* By adding polynomial features (e.g.,x², x³).
* Use L2 regularization or cross-validation to select the optimal degree


Question:
* What is the difference between Grid Search and Random Search?
* What are the advantages of Bayesian Optimization?

Answer:
* Grid Search exhaustively tests all parameter combinations, while Random Search samples randomly (more efficient for high-dimensional spaces).
* Bayesian Optimization uses historical data to dynamically adjust search ranges.



### Imbalanced Data

Question:
* How to address class imbalance in classification?
* How to handle outliers in regression?

Answer:
* Use SMOTE (oversampling), class weights, or F1-score evaluation.
* Remove outliers or use robust regression (e.g., Huber Loss).


### Linear Discriminant Analysis (LDA)
* Core Idea: Find projections maximizing between-class variance and minimizing within-class variance for dimensionality reduction and classification.
* Assumptions:
    * Features follow Gaussian distributions.
    * Equal class covariance matrices.
* Advantages:
    * Supervised dimensionality reduction.
    * Efficient for linearly separable data.
* Disadvantages:
    * Poor performance on nonlinear data.
    * Strict distributional assumptions.

Question: Difference between LDA and PCA?

Answer:
* LDA is supervised (uses class labels), while PCA is unsupervised.
* LDA maximizes class separability, PCA maximizes data variance.
* LDA’s output dimensions ≤ classes−1, PCA has no such limit.

Question: What are the assumptions of LDA?

Answer: Gaussian features, equal covariance matrices, independent samples.

Question: Difference between LDA and QDA?

Answer: QDA allows unequal covariance matrices (flexible for nonlinear data) but is prone to overfitting.

### Support Vector Machine (SVM)
* Core Idea: Find the hyperplane with maximum margin, using kernel tricks for nonlinear data.
* Key Concepts:
    * Kernels: Map data to higher dimensions (e.g., RBF, polynomial).
    * Soft Margin: Allow misclassifications controlled by the penalty parameter C.
* Advantages:
    * Effective for small high-dimensional datasets.
    * Strong theoretical foundation.
* Disadvantages:
    * High computational cost (especially with kernels).
    * Parameter sensitivity.

Question: Why is SVM suitable for small high-dimensional data?

Answer: Kernels efficiently handle high dimensions without the curse of dimensionality, and margin maximization provides robustness.

Question: How to choose SVM kernels?

Answer:
* Linear: For linearly separable data (e.g., text).
* RBF: Default choice for nonlinear data.
* Polynomial: Complex and prone to overfitting.

Question: How to handle imbalanced data in SVM?

Answer: Adjust class_weight, use SMOTE, or cost-sensitive learning.

Question: What is the dual problem in SVM? Why is it needed?

Answer: The dual form transforms the optimization problem to enable kernel usage and handle high-dimensional data.