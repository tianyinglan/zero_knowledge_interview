# fft
### Optimization of Polynomial Calculation (POLY)
1. Recursive Decomposition NTT Algorithm
    * Principle: Decompose large-scale Number Theoretic Transform (NTT) into multiple small-scale NTT cores to reduce the pressure of off-chip memory access.
        * Decomposition method: Recursively decompose a large NTT (e.g., 1 million points) into multiple small NTTs (e.g., 1024 points), using a divide-and-conquer strategy to reduce computational complexity.
        * Hardware implementation: Design a lightweight NTT module. Achieve step-size access in different stages through FIFO, avoiding complex multiplexers.
    * Advantages:
        * Reduce the off-chip memory bandwidth requirement (from 2.98 TB/s to 5.96 GB/s).
        * Support dynamic adjustment of the core size to adapt to different application scenarios.
2. Data Blocking and On-Chip Matrix Transposition
    * Data blocking: Read input data in blocks to improve the continuity of off-chip memory access.
    * Matrix transposition: Convert row-major data to column-major through on-chip SRAM buffering, optimizing the memory access pattern for NTT calculation.
    * Effects:
        * Reduce bandwidth waste caused by Strided access.
        * Support parallel processing of multiple NTT modules to enhance throughput.

### Principles of FFT Hardware Acceleration
1. Parallel Computing Architecture
    * SIMD Instruction Set: Utilizes Single Instruction Multiple Data (SIMD) technology. For instance, the CUDA core of an NVIDIA GPU supports 128-bit/256-bit vector operations, breaking down complex multiplications into 4–8 floating-point operations executed concurrently.
    * Pipeline Processing: Splits FFT butterfly operations into multiple pipelined stages (like rotation factor multiplication, addition, and subtraction), with each stage handling different data in parallel.
2. Memory Hierarchy Optimization
    * Block Processing: Divides large arrays into L1 cache-sized blocks (e.g., 16KB) to reduce global memory accesses.
    * Transposition Optimization: Achieves in-place transposition via shared memory, avoiding redundant data movement. For example, in 2D-FFT, each thread block processes a sub-matrix and uses shared memory to complete row-column exchanges.
3. Algorithm-Level Parallelism
    * Multithreading Parallelism: Each CUDA thread handles independent butterfly operations. For an N=2^m-point FFT, N threads are initiated for parallel processing.
    * Recursive Divide and Conquer: Breaks FFT into smaller subproblems (like radix-2 or radix-4 algorithms). Combined with hardware warp scheduling, it enables task parallelism.
4. Dedicated Hardware Units
    * Tensor Core Acceleration: The Tensor Core of an NVIDIA A100 GPU accelerates complex multiplications, implementing FFT butterfly operations through matrix multiplication instructions.
    * Cache Prefetching: The hardware prefetch unit loads data into L2 cache ahead of time to hide memory latency.


### FFT Optimization
Advantages of Radix-2/Radix-4 Hybrid Algorithm and Its CUDA Implementation
* Advantages:
    * Reduced Multiplications: Radix-4 stages use fewer complex multiplications per butterfly operation compared to radix-2.
    * Improved Parallelism: Hybrid algorithms balance recursion depth and computational efficiency for better GPU utilization.
    * Scalability: Dynamically switch between radix-2 and radix-4 based on input size (e.g., use radix-4 for large N, radix-2 for small N).

CUDA Implementation:
__global__ void hybridFFT(float* data, int N) {  
    int i = threadIdx.x;  
    if (i >= N) return;  
    // Radix-4 butterfly for large N  
    if (N >= 1024) {  
        // Implement radix-4 decomposition and twiddle factor multiplication  
        // ...  
    } else {  
        // Radix-2 fallback for small N  
        // ...  
    }  }  

Detailed Steps for 2D-FFT Transposition Optimization with Shared Memory Double Buffering
1. Data Loading:
    * Load matrix blocks from global memory to the first shared memory buffer.
    * Ensure coalesced access by aligning thread indices with memory offsets.
    
    __shared__ float buffer1[32][32];  int idx = threadIdx.x + threadIdx.y * blockDim.x;  
    buffer1[threadIdx.y][threadIdx.x] = data[blockIdx.x * blockDim.x * blockDim.y + idx];  
2. Transposition:
    * Swap row and column indices in shared memory.
    
    __syncthreads();  float temp = buffer1[threadIdx.y][threadIdx.x];  
    buffer1[threadIdx.x][threadIdx.y] = temp;  
3. Data Writing:
    * Write transposed data back to global memory.
    
    __syncthreads();  
    data[blockIdx.x * blockDim.x * blockDim.y + threadIdx.x + threadIdx.y * blockDim.x] = buffer1[threadIdx.y][threadIdx.x];  
4. Double Buffering Switch:
    * While processing the first buffer, load the next block into the second buffer.

    __shared__ float buffer2[32][32];  if (blockIdx.x < gridDim.x - 1) {  
    buffer2[threadIdx.y][threadIdx.x] = data[(blockIdx.x + 1) * blockDim.x * blockDim.y + idx];  }  
5. Repeat：
    * Alternate between buffers to overlap computation and memory transfer.
    
    __syncthreads();  // Repeat transposition for buffer2  
* Key Optimizations:
    * Double Buffering: Overlaps data loading with computation using two shared memory buffers.
    * Bank Conflict Avoidance: Use padding (e.g., 32×32 + 1) to prevent shared memory bank conflicts during transposition.
    * Memory Coalescing: Ensure row-major/column-major alignment for global memory access.

### Performance Comparison:

| Method            | 2D-FFT Latency (RTX 4090) | Bandwidth Utilization |
| ----------------- | ------------------------- | --------------------- |
| Naive Transposition | 120 μs                    | 40%                   |
| Shared Memory     | 28 μs                     | 82%                   |
| Double Buffering  | 19 μs                     | 95%                   |

### Best Practices:
    * Use 32×32 tiles to match GPU SM capabilities.
    * Combine with CUFFT’s built-in transposition for optimal performance.
    * Prefer static allocation for shared memory to minimize runtime overhead.

### Algorithm Selection
* For small point numbers (N<1024): Radix-4/Radix-8 hybrid algorithm.
* For large point numbers (N>1024): Divide-and-conquer FFT combined with a hybrid radix algorithm.
* For 2D-FFT: Convert into two 1D-FFTs for rows/columns, utilizing transposition optimization.


### Principles of the Radix-4/Radix-8 Hybrid FFT Algorithm
1. Core of the Radix-4 Algorithm
    * Decomposition method: Divide the N-point sequence into 4 sub-sequences and recursively decompose them into N/4-point FFT.
    * Butterfly operation: Each radix-4 stage contains 3 complex multiplications (reducing the number of multiplications by 50% compared to radix-2).
    * Rotation factor: Use ω^k = e^(-2πi k/N)，k=0,1,2,3
* Design of the Divide-and-Conquer Architecture
    * Recursive termination condition: Switch to the radix-2 or radix-4 algorithm when the sub-problem size ≤32.
    * Task partitioning: Decompose the large N-point FFT into multiple subtasks (e.g., N=2048→4×512-point FFT).
    * Pipeline processing: Execute the preceding decomposition stage and the subsequent merging stage in parallel.
    * Thread allocation: Allocate independent thread blocks for each subtask.
    * Register reuse: Store intermediate results via register files to reduce shared memory access.
    * Branch prediction optimization: Eliminate redundant branches using conditional compilation (e.g., using #ifdef RADIX_8).
* 2D-FFT Transposition Optimization Technology
    * Row-Column Decomposition Strategy
    * Row processing stage: Independently execute 1D-FFT for each row.
    * Matrix transposition: Convert row-major storage to column-major.
    * Column processing stage: Execute 1D-FFT for each column.
    * Reverse transposition: Restore the original storage order.
* Implementation of Transposition Optimization
    * Shared memory blocking: Divide the large matrix into 32×32 sub-blocks, with each thread block handling one sub-block.
    * Double-buffering technology.
* Key Technical Challenges
    * Dynamic load balancing: The divide-and-conquer strategy must dynamically adjust task allocation based on hardware status.
    * Mixed-precision support: Maintain numerical stability in low-precision calculations (e.g., using FP16+FP32 mixed precision).
    * Cross-platform adaptation: Optimize the rotation factor table for different GPU architectures (e.g., Ampere vs. Hopper).

### TT (Number Theoretic Transform) Technical Solutions and Optimization Ideas
1. Technical Solutions
    * Algorithm Principle: Based on a 4-step recursive algorithm, decompose 2²⁴-point NTT into multiple 2¹²-point NTTs, achieved through column and row transformations:
        * Matrix Layout: Arrange 2²⁴ points into a 2¹²×2¹² matrix (row-major order).
        * Column NTT: Execute 2¹²-point NTT for each column.
        * Twiddle Factor Multiplication: Correct the results after column transformation.
        * Row NTT: Execute 2¹²-point NTT for each row, implicitly transposing the matrix.
    * Hardware Implementation:
        * Parallel Cores: Support 8/16/32/64 cores, with each core handling 2¹²-point NTT.
        * Pipelined Structure: Use AXI stream for data transmission, and HLS cores to schedule PCIe and HBM access.
        * Dynamic Memory Access: Optimize HBM bandwidth through pre/post-processing, supporting 2048–4096-byte burst transfers.
2. Optimization Ideas
    * Recursive Decomposition:
        * Decompose large NTT into small cores, reducing off-chip memory bandwidth requirements (from 2.98 TB/s to 5.96 GB/s).
    * Data Blocking and Transposition:
        * Buffer data in on-chip SRAM to reduce Strided access.
        * Achieve optimized memory layout through host preprocessing to improve HBM bandwidth utilization.
    * Pipeline Parallelism:
        * Multiple cores process column/row transformations in parallel to mask computation and transmission latency.
    * Efficient Hardware Resource Utilization:
        * Each core uses distributed RAM and butterfly units to reduce routing complexity.
        * Optimize Twiddle factor generation to reduce multiplier resource consumption.
    * Special Value Handling:
        * Support different window sizes (e.g., 2¹² points) to flexibly adapt to various scenarios.